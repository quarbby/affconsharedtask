{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"\"report\".ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Z2rqQeGbafjL","colab_type":"text"},"source":["#Intro\n","\n","Problem: Given sentences sourced from comment text from posts that contain certain tags on r/OffMyChest and r/CasualConversations, determine if sentence is supportive and/or disclosive\n","\n","Define inputs/outputs  \n","Model choices  \n","- Classical Models  \n","    - Logistic\n","    - Support Vector Machine\n","    - Stochastic Gradient Descent\n","    - Naive Bayesian Inference\n","\n","These models take in vectors that are created from the words, which include tfidf vectors, LSA vectors based off the tfidf vectors and LIWC vectors, which is just a vector of frequencies for every category in LIWC.\n","\n","The outputs are just the probabilities for each label\n","\n","I used 10 fold cross validation for these, and measured the per label and micro average precision, recall and f1 score\n","\n","- Neural Network based Models\n","    - Simple Feedforward\n","    - AWD-LSTM\n","    - Transformer based networks\n","        - (Distil) RoBERTa (distilled version and not are basically interchangeable)\n","        - XLNet\n","\n","The simple NN takes the feature vectors as input (excluding tfidf as those are too big), while the other models take in text. To be more specific, the text actually is tokenised first into tokens that the model was trained on, then numericalised according to the vocabulary of the model. \n","\n","The outputs of the models are logits, which need to be passed through a sigmoid function in order to get the actual probabilities.\n","\n","I left out 10% of labeled data to use as a validation set, and also measured the per label and micro average precision, recall and f1 score. Some of the results also contained the 'accuracy', the total number of correct labels over the total number of labels\n","\n","Also, every mention of RoBERTa will most likely be the distilled version due to colab memory constraints\n","\n","\n","- Language Model Training\n","    - (Distil) RoBERTa\n","\n","Uses the unlabeled comments as text in order to do masked language modelling training, where words in the sentences will be randomly replaced with mask tokens, which the model will then have to predict.\n","\n","Outputs the logits? for the possible classes for each word position. \n","\n","The RoBERTa part of the model is then loaded instead of the provided pretrained model to be used for classification. \n","\n","- [Unsupervised Data Augmentation](https://arxiv.org/pdf/1904.12848.pdf)\n","    - Backtranslation\n","        - [Facebook's submission to WMT'19](https://github.com/pytorch/fairseq/blob/master/examples/wmt19/README.md) \n","    - (Distil) RoBERTa\n","\n","The gist of UDA is basically consistency training using the data, but instead of adding noise, we use data augmentation as the source of noise. I choose to use back-translation as opposed to tfidf word replacement as the augmentation of choice as the task at hand is not really dependent on any particular key words, but rather the whole sentence structure as a whole (which is why I felt that the features would not be able to predict the classes very well), so tfidf based word replacement would generate a less diverse paraphrasing. \n","\n","To go into more detail, we compute a combined loss consisting of a supervised and unsupervised portion, where the supervised loss is a standard loss comparing the outputs of the model to the labels, while the unsupervised portion is the (KL) divergence between the predictions of the original unlabeled text compared to the backtranslated text (the distribution we are comparing to is the original). But since our problem is multilabel, I have adapted the KL divergence to compute the divergence between the distribution of every class (so calculate the KL divergence for the two probabilties for every class). I suspect that simply using a binary cross entropy meant for multilabeled data would give similar, or even better results since KL divergence doesnt make for a good loss (according to the internet as far as I can tell)\n","\n","There is also the Training Signal Annealing portion of UDA, where if the model's confidence on a particular training example is above a certain threshold determined by the stage of the training the model is at, it does not count toward the supervised loss. According to the authors of the paper, this was their strategy for dealing with overfitting. More details in [paper](https://arxiv.org/pdf/1904.12848.pdf). \n","\n","The unlabeled comment text is fed into facebook's EN-GE and then GE-EN translation model in order to generate paraphrases for UDA. I chose to use Facebook's fairseq instead of tensor2tensor as in the original paper as tensor2tensor was giving me issues with dropping translations as well as having a older model. \n","\n","- Feature permutation importance\n","\n","Basically for every liwc feature we swap the first half and the second half of the feature to see what is the drop in error. This will allow us to see which category in LIWC affects the neural network the most"]},{"cell_type":"markdown","metadata":{"id":"-Y40by8hamMY","colab_type":"text"},"source":["#Results\n","\n","Table form? and discussion"]},{"cell_type":"markdown","metadata":{"id":"F8j1XwKPC4gO","colab_type":"text"},"source":["##Classical Classifiers\n","###Data\n","Format:   \n","```\n","                supportive    disclosive  \n","precision  \n","recall  \n","f1 score  \n","support(ignore)  \n","\n","precision recall f1 score  \n","```\n","\n","OneVsRest  \n",">Tfidf:  \n","Results for classifier Logistic Reg  \n","[[  0.741242   0.578209]  \n"," [  0.082481   0.69604 ]  \n"," [  0.147563   0.631616]  \n"," [322.6      681.9     ]]  \n","[0.584943 0.499209 0.538674]    \n","Results for classifier LinearSVC  \n","[[  0.58721    0.584152]  \n"," [  0.179531   0.603428]  \n"," [  0.274463   0.593333]  \n"," [322.6      681.9     ]]  \n","[0.584547 0.467203 0.519199]    \n","Results for classifier SGD  \n","[[  0.698698   0.579263]  \n"," [  0.122572   0.673083]  \n"," [  0.208287   0.622461]  \n"," [322.6      681.9     ]]  \n","[0.587186 0.496412 0.537812]  \n","Results for classifier Naive Bayes  \n","[[  0.929048   0.54631 ]  \n"," [  0.013076   0.893738]  \n"," [  0.025744   0.677871]  \n"," [322.6      681.9     ]]  \n","[0.547815 0.610792 0.577488]  \n","\n",">LSA\n","Results for classifier Logistic Reg  \n","[[  0.683801   0.570369]  \n"," [  0.073255   0.728009]  \n"," [  0.131953   0.639117]  \n"," [322.6      681.9     ]]  \n","[0.574782 0.517624 0.544436]    \n","Results for classifier LinearSVC  \n","[[  0.643912   0.587667]  \n"," [  0.113716   0.642443]  \n"," [  0.193103   0.613607]  \n"," [322.6      681.9     ]]  \n","[0.591656 0.472631 0.525411]    \n","Results for classifier SGD  \n","[[  0.79091    0.541212]  \n"," [  0.05234    0.931395]  \n"," [  0.097841   0.677928]  \n"," [322.6      681.9     ]]  \n","[0.545714 0.64887  0.587517]  \n","Results for classifier Naive Bayes  \n","Naive Bayes doesnt like matrix factorisation methods (like LSA)\n","\n",">LIWC\n","Results for classifier Logistic Reg  \n","[[  0.598628   0.631613]  \n"," [  0.227055   0.680657]  \n"," [  0.328835   0.65489 ]  \n"," [322.6      681.9     ]]  \n","[0.626635 0.534994 0.577055]    \n","Results for classifier LinearSVC  \n","[[  0.590914   0.6332  ]  \n"," [  0.227962   0.668885]  \n"," [  0.32873    0.650281]  \n"," [322.6      681.9     ]]  \n","[0.626825 0.527367 0.572623]  \n","Results for classifier SGD  \n","[[  0.661719   0.658957]  \n"," [  0.02916    0.609686]   \n"," [  0.054394   0.623961]  \n"," [322.6      681.9     ]]  \n","[0.657101 0.423416 0.507834] \n","Results for classifier Naive Bayes    \n","[[  0.         0.559526]  \n"," [  0.         0.892687]  \n"," [  0.         0.687544]  \n"," [322.6      681.9     ]]  \n","[0.559526 0.605887 0.581585]  \n","\n"]},{"cell_type":"markdown","metadata":{"id":"Fa_NxINtHS27","colab_type":"text"},"source":["###Comments\n","\n","Standard baseline, worse results than I expected initially as other multilabel text classification problems had better results, even with the classical models. Also prone to classifying everything the same (noticed some ill defined f1 scores and precision scores), and lower recall that may be caused by data imbalance (not many support labels)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6IS9pN8uQ81f","colab_type":"text"},"source":["## AWD-LSTM\n","\n","### Data\n","```\n","precision    recall  f1-score   support\n","\n","     support    0.67552   0.33578   0.44858       682\n","  disclosive    0.67643   0.69581   0.68598      1361\n","\n","   micro avg    0.67625   0.57562   0.62189      2043\n","   macro avg    0.67597   0.51579   0.56728      2043\n","weighted avg    0.67612   0.57562   0.60673      2043\n"," samples avg    0.43121   0.40770   0.41340      2043\n","```"]},{"cell_type":"markdown","metadata":{"id":"c5d2oMJbRFTv","colab_type":"text"},"source":["###Comments\n","\n","Better than the classical models, but suffering from a lower support recall as well. Attempted to use ULMFiT, but only to find out I did it wrong afterwards (Used a constant learning rate for all the layers instead of decreasing ones). Not much else to say, as I did not test it very thoroughly. \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"r3O2CBFYcSaP","colab_type":"text"},"source":["## XLNet and RoBERTA with the provided pretrained models\n","\n","###Data\n","```\n","RoBERTa:  \n","              precision    recall  f1-score   support\n","\n","     support    0.67365   0.67164   0.67265       335\n","  disclosive    0.69572   0.69881   0.69726       674\n","\n","   micro avg    0.68843   0.68979   0.68911      1009\n","   macro avg    0.68468   0.68523   0.68495      1009\n","weighted avg    0.68839   0.68979   0.68909      1009\n"," samples avg    0.47784   0.47589   0.46967      1009\n","\n","\n","XLNet:\n","\n","              precision    recall  f1-score   support\n","\n","     support    0.63714   0.66567   0.65109       335\n","  disclosive    0.69913   0.71365   0.70631       674\n","\n","   micro avg    0.67823   0.69772   0.68784      1009\n","   macro avg    0.66814   0.68966   0.67870      1009\n","weighted avg    0.67855   0.69772   0.68798      1009\n"," samples avg    0.48523   0.48289   0.47538      1009\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"52n0L1YAgVJJ","colab_type":"text"},"source":["###Comments\n","\n","Much better results than previous models, and the support recall does not seem to be significantly lower anymore. The precision and recall of the RoBERTa model seem to be more balanced as compared to for the XLNet model. There is still a slight discrepancy between the metrics for support vs disclosure, which is more prominent in the case for XLNet."]},{"cell_type":"markdown","metadata":{"id":"hT05vulKe9u6","colab_type":"text"},"source":["## Pretrained on unlabeled comments\n","\n","###Data\n","##### lm 3:\n","###### best valid loss was kept:\n",">lr:2e-5,wd:0.001 \n","(0.7167042889390519, 0.6293359762140733, 0.670184696569921, None)\n","0.5754276827371695\n","\n",">lr:1e-5,wd:0.001 \n","(0.678963110667996, 0.6749256689791873, 0.6769383697813122, None)\n","0.562208398133748\n","\n",">lr:1e-4,wd:0.001  \n","(0.6697626418988648, 0.643211100099108, 0.6562184024266936, None)\n","0.5427682737169518\n","\n",">lr:5e-5,wd:0.001  \n",">(0.6740597878495661, 0.6927651139742319, 0.6832844574780058, None)\n",">0.5598755832037325\n","\n",">lr:1e-3,wd:0.001  \n",">(0.5241057542768274, 0.66798810703667, 0.587363834422658, None)\n",">0.42068429237947125\n","\n","###### best f1 score was kept:\n",">lr:2e-5,wd:0.001  \n","(0.6678798908098271, 0.7274529236868187, 0.6963946869070209, None)\n","0.5668740279937792\n","\n",">lr:1e-5,wd:0.01   \n","(0.6825242718446602, 0.6967294350842418, 0.689553702795488, None)\n","0.567651632970451\n","\n","##### lm 6:  \n","Seems worse overall over the many attempts (not recorded)\n","lr:7e-5,wd:0.001\n","(0.698051948051948, 0.6392467789890981, 0.6673564407656492, None)\n","0.5544323483670296\n","\n","##### lm 7:  \n",">lr:2e-5,wd:0.001  \n","(0.6724303554274735, 0.6937561942517344, 0.6829268292682927, None)\n","0.5544323483670296\n","\n",">lr:1e-4,wd:0.001  \n","(0.6835978835978836, 0.6402378592666006, 0.6612077789150461, None)\n","0.5443234836702955\n","\n",">lr:1e-5,wd:0.001  \n","(0.6734892787524367, 0.6848364717542121, 0.6791154791154792, None)\n","0.5559875583203733\n","\n",">lr:9e-6,wd:0.001  \n","(0.6902564102564103, 0.6669970267591675, 0.6784274193548386, None)\n","0.5645412130637636\n","\n",">lr:5e-6,wd:0.001  \n","(0.6837944664031621, 0.6858275520317145, 0.6848095002474023, None)\n","0.5637636080870918\n","\n",">lr:1e-6,wd:0.001 (more epochs than usual)  \n","(0.6829025844930418, 0.6808721506442021, 0.6818858560794046, None)\n","0.5668740279937792\n","\n",">lr:2e-6,wd:0.001\n","(0.6791120080726539, 0.6669970267591675, 0.673, None)\n","0.5528771384136858\n","\n","##### lm 8:  \n","\n",">lr:5e-5,wd:0.001  \n","(0.7510489510489511, 0.5322101090188305, 0.622969837587007, None)\n","0.5497667185069984\n","\n",">lr:2e-5,wd:0.001  \n","(0.6762452107279694, 0.6997026759167493, 0.6877739892839747, None)\n","0.5552099533437014\n","\n",">lr:1e-5,wd:0.001  \n","(0.6759615384615385, 0.6967294350842418, 0.6861883845778428, None)\n","0.5629860031104199\n","\n",">lr:9e-6,wd:0.001  \n","(0.7004264392324094, 0.6511397423191279, 0.6748844375963019, None)\n","0.5699844479004665\n","\n",">lr:1e-4,wd:0.001  \n","(0.7113133940182055, 0.5421209117938554, 0.6152980877390326, None)\n","0.5427682737169518\n","\n",">lr:1e-6,wd:0.001  \n","(0.6923879040667362, 0.6580773042616452, 0.6747967479674797, None)\n","0.5660964230171073\n","\n",">lr:5e-6,wd:0.001  \n","(0.6791907514450867, 0.6987115956392468, 0.6888128969223253, None)\n","0.5684292379471229\n","\n","##### lm 9:  \n",">lr:1e-5,wd:0.001  \n","(0.6918429003021148, 0.6808721506442021, 0.6863136863136863, None)\n","0.5754276827371695\n","\n",">lr:2e-5,wd:0.001  \n","(0.6914778856526429, 0.6352824578790882, 0.6621900826446281, None)\n","0.5544323483670296\n","\n",">lr:9e-6,wd:0.001  \n","(0.6778523489932886, 0.7006937561942518, 0.689083820662768, None)\n","0.562208398133748\n","\n",">lr:5e-6,wd:0.001  \n","(0.7067099567099567, 0.647175421209118, 0.6756337299534403, None)\n","0.5746500777604977\n","\n",">lr:2e-6,wd:0.001  \n","(0.6854838709677419, 0.6739345887016849, 0.6796601699150424, None)\n","0.5637636080870918\n","\n",">lr:1e-6,wd:0.001  \n","(0.6786786786786787, 0.6719524281466799, 0.6752988047808764, None)\n","0.557542768273717"]},{"cell_type":"markdown","metadata":{"id":"uJ4m6yDMFwW9","colab_type":"text"},"source":["### Comments\n","\n","I only pretrained the RoBERTa model as it was easier to understand which aspect of language it was modelling, which led me to start implementing it first as a test, but then when the results were worse I decided it was not worth the effort to try to implement the permutation modelling of XLNet (and the cancer looking 2 streams of attention it boasts). The fact that there was sample code on the huggingface docs also helped in making this decision.\n","\n","I had thought that these models would give better results than the previous, but it seems to perform slightly worse, even under extensive parameter tuning. This may be due to the fact that I did not train the language model with [discriminative finetuning](https://arxiv.org/pdf/1801.06146.pdf), where the learning rates vary from layer to layer. \n"]},{"cell_type":"markdown","metadata":{"id":"y24zovk0TpQg","colab_type":"text"},"source":["## Simple feedforward network using features\n","\n","###Data\n","\n","\n","LSA:\n","```\n","              precision    recall  f1-score   support\n","\n","     support    0.59677   0.22090   0.32244       335\n","  disclosive    0.60345   0.57122   0.58689       674\n","\n","   micro avg    0.60236   0.45491   0.51835      1009\n","   macro avg    0.60011   0.39606   0.45467      1009\n","weighted avg    0.60123   0.45491   0.49909      1009\n"," samples avg    0.34176   0.31921   0.32426      1009\n","```\n","LIWC:  \n","```\n","              precision    recall  f1-score   support\n","\n","     support    0.56757   0.43881   0.49495       335\n","  disclosive    0.64613   0.60682   0.62586       674\n","\n","   micro avg    0.62332   0.55104   0.58496      1009\n","   macro avg    0.60685   0.52282   0.56041      1009\n","weighted avg    0.62005   0.55104   0.58240      1009\n"," samples avg    0.39425   0.38608   0.38362      1009\n","```"]},{"cell_type":"markdown","metadata":{"id":"Yf1Jz7zFT2SA","colab_type":"text"},"source":["###Comments\n","\n","Recall issue is present here. This is better than the classical classifiers but worse than the LSTM based one."]},{"cell_type":"markdown","metadata":{"id":"p_0-kCc5R31e","colab_type":"text"},"source":["## UDA\n","\n","### Data\n","#### With finetuned language model\n","log:  \n","lr:1e-5  \n","```\n","\n","              precision    recall  f1-score   support\n","\n","     support    0.68038   0.64179   0.66052       335\n","  disclosive    0.66922   0.77745   0.71929       674\n","\n","   micro avg    0.67243   0.73241   0.70114      1009\n","   macro avg    0.67480   0.70962   0.68990      1009\n","weighted avg    0.67293   0.73241   0.69978      1009\n"," samples avg    0.50778   0.50855   0.49974      1009\n","\n","epoch\ttrain_loss\tvalid_loss\taccuracy\taccuracy_thresh\tmulti_label_fbeta\tcpu used\tpeak\tgpu used\tpeak\ttime\n","0\t0.619644\t0.535592\t0.393079\t0.726672\t0.679435\t40\t75\t1132\t13850\t19:54\n","1\t0.575193\t0.508279\t0.387636\t0.750778\t0.692861\t20\t53\t0\t13618\t19:52\n","2\t0.592565\t0.506151\t0.386081\t0.752722\t0.663848\t41\t75\t518\t13720\t13:48\n","3\t0.575860\t0.504516\t0.381415\t0.749222\t0.698739\t20\t50\t0\t13662\t13:49\n","4\t0.590860\t0.501973\t0.384526\t0.755054\t0.701139\t20\t54\t0\t13744\t13:51\n","```\n","\n","exp:  \n","lr 1e-5\n","```\n","              precision    recall  f1-score   support\n","\n","     support    0.68182   0.62687   0.65319       335\n","  disclosive    0.65380   0.77893   0.71090       674\n","\n","   micro avg    0.66157   0.72844   0.69340      1009\n","   macro avg    0.66781   0.70290   0.68204      1009\n","weighted avg    0.66310   0.72844   0.69174      1009\n"," samples avg    0.51089   0.50700   0.50026      1009\n","\n","\n"," epoch\ttrain_loss\tvalid_loss\taccuracy\taccuracy_thresh\tmulti_label_fbeta\tcpu used\tpeak\tgpu used\tpeak\ttime\n","0\t0.493801\t0.680391\t0.396190\t0.691291\t0.679580\t31\t62\t1132\t13784\t13:49\n","1\t0.574188\t0.663418\t0.403966\t0.747667\t0.673377\t20\t55\t0\t13836\t13:56\n","2\t0.574881\t0.630371\t0.393079\t0.746112\t0.687709\t20\t52\t0\t13812\t13:53\n","3\t0.593605\t0.560605\t0.392302\t0.749611\t0.691571\t20\t50\t0\t13818\t13:44\n","4\t0.558335\t0.521513\t0.387636\t0.747278\t0.693396\t20\t54\t0\t13776\t13:41\n","```"]},{"cell_type":"markdown","metadata":{"id":"U5PXqr9rVFV4","colab_type":"text"},"source":["```\n","log: \n","lr 1e-5\n","epoch\ttrain_loss\tvalid_loss\taccuracy\taccuracy_thresh\tmulti_label_fbeta\tcpu used\tpeak\tgpu used\tpeak\ttime\n","0\t0.648394\t0.543909\t0.388414\t0.737947\t0.682375\t20\t53\t1132\t13834\t13:53\n","1\t0.595730\t0.522705\t0.381415\t0.732115\t0.685532\t20\t50\t0\t13782\t13:52\n","2\t0.596397\t0.517787\t0.386858\t0.751555\t0.668396\t20\t54\t0\t13724\t13:51\n","3\t0.551665\t0.526049\t0.393857\t0.729393\t0.683060\t20\t52\t0\t13844\t13:56\n","4\t0.531672\t0.529591\t0.391524\t0.735225\t0.685160\t20\t56\t0\t13840\t14:00\n","\n","              precision    recall  f1-score   support\n","\n","     support    0.75120   0.46866   0.57721       335\n","  disclosive    0.68688   0.72255   0.70427       674\n","\n","   micro avg    0.70153   0.63826   0.66840      1009\n","   macro avg    0.71904   0.59560   0.64074      1009\n","weighted avg    0.70824   0.63826   0.66208      1009\n"," samples avg    0.46229   0.44751   0.44816      1009\n","```\n","\n","exp:  \n","1e-5  \n","```\n","              precision    recall  f1-score   support\n","\n","     support    0.65823   0.62090   0.63902       335\n","  disclosive    0.66232   0.75371   0.70507       674\n","\n","   micro avg    0.66113   0.70961   0.68451      1009\n","   macro avg    0.66027   0.68730   0.67204      1009\n","weighted avg    0.66096   0.70961   0.68314      1009\n"," samples avg    0.49806   0.49456   0.48808      1009\n","\n","epoch\ttrain_loss\tvalid_loss\taccuracy\taccuracy_thresh\tmulti_label_fbeta\tcpu used\tpeak\tgpu used\tpeak\ttime\n","0\t0.488590\t0.675085\t0.386081\t0.664852\t0.645850\t20\t52\t1132\t13832\t13:53\n","1\t0.558965\t0.663746\t0.394635\t0.741446\t0.631579\t20\t55\t0\t13796\t13:52\n","2\t0.543570\t0.636842\t0.408631\t0.740280\t0.607981\t20\t53\t0\t13846\t13:58\n","3\t0.566829\t0.565225\t0.398523\t0.747667\t0.667009\t20\t51\t0\t13768\t13:48\n","4\t0.550432\t0.529440\t0.393079\t0.743390\t0.684512\t20\t55\t0\t13830\t13:46\n","```"]},{"cell_type":"markdown","metadata":{"id":"2h4D1ooDSI9H","colab_type":"text"},"source":["###Comments\n","\n","Gives comparable results to the given pretrained models, but without any parameter finetuning at all. I feel like better results could be obtained with further adjustment of the parameters. The language model finetuned on the unlabeled text seems to give better results when paired with UDA."]},{"cell_type":"markdown","metadata":{"id":"-Qs3_VXSTigB","colab_type":"text"},"source":["##Feature Importance\n","\n","Look in notebook for more data\n","\n","###Comments\n","\n","Did not manage to test much but I noticed that adj was a influential category for the predictions made by the simple feedforward network.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5P3eatRGarnX","colab_type":"text"},"source":["#Discussion\n","\n","Say issues and thought process  \n","Explain what was tried"]},{"cell_type":"markdown","metadata":{"id":"FnDseqJTmuOW","colab_type":"text"},"source":["##Implementation of things\n","\n","Used fast.ai as it seemed to be easy to use, and it had some 1cycle training scheduling method that had results behind it.\n","\n","The problem is fast.ai is very annoying to customise and had some issues that required some hacky workarounds\n","\n","I haven't really looked too closely but fastai2 seems to fix these issues"]},{"cell_type":"markdown","metadata":{"id":"gdUC4NAFqUgy","colab_type":"text"},"source":["### Data Cleaning\n","\n","Data came with unicode artefacts, presumably from improper processing in R (as the text contained stuff liek <U+0009>, which I could only find in R) so had to regex that into the proper character. Also had to remove markdown and html entities as the comments were from reddit. "]},{"cell_type":"markdown","metadata":{"id":"RjQ4aWhJqN84","colab_type":"text"},"source":["###Features\n","\n","To generate the features I lemmatized the text before putting it through either a tfidf vectorizer, or i got the frequency of every liwc category, which served as a vector. The LSA vectors were derived from the tfidf vectors as well."]},{"cell_type":"markdown","metadata":{"id":"SSfVgKsrqENb","colab_type":"text"},"source":["###Classical Classifiers\n","\n","Simple application of sklearn, nothing special was done here"]},{"cell_type":"markdown","metadata":{"id":"R7F4n8BssfPY","colab_type":"text"},"source":["### Fast.ai stuff\n","\n","#### Quick rundown on fastai\n","\n","Fastai modularises the whole pipeline into the dataprocessing to create a so called DataBunch, and the training and testing using a Learner. Learner can take in pytorch modules. Callbacks allow you to customise things more easily by having a callback before and after basically every step of training loop. They also have [1cycle](https://arxiv.org/pdf/1803.09820.pdf) implemented, which seems to do the same or better in terms of metrics using a shorter amount of time.\n","\n","####Rough Explanation\n","\n","LSTM was inbuilt, so the implementation is just following the tutorial and documentation\n","\n","For the transformers, I decided to use the [pytorch implementation by huggingface team](https://huggingface.co/transformers/) since it was the most popular and hence well maintained port in pytorch. However, since our problem is a multilabel one, I had to use a [different library](https://github.com/kaushaltrivedi/fast-bert/blob/master/fast_bert/) that built on top of the huggingface library that implemented a multilabel classification head.\n","\n","There were also quite a few tutorials online that had integrated the two libraries which I followed.  \n","https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2  \n","https://github.com/maximilienroberti/fastai-transformers/blob/master/Fastai_%2B_Transformers_%2B_ULMFiT.ipynb  \n","The gist of it is basically wrap the huggingface tokenizers in fastai versions so that it fits into the text preprocessing pipeline that already exists. Then we need to wrap the huggingface model as it gives a tuple as output instead of just the logits we predicted. (on hindsight we can get around this using a callback)\n","\n","To train the RoBERTa language model, I had to implement masked language modelling in fastai. This involved masking the input text before feeding it into the model, and storing which tokens were masked. This was done using a callback, and labelling the data with an empty label. To export this model, I used the huggingface export as that allows the model to be transferred without the head (aka classifier linear layer or language modelling head).\n","\n","##### [UDA](https://)\n","Ok this was a massive effort (and there were no tutorials :/)\n","\n","Every batch into the model had to consist of labeled and unlabeled data. This was a problem as the labeled data were labeled with 'one hot encoded' vectors (quotes as multilabel so not one hot per se), while the unlabeled data was 'labeled' with text (quotes as not exactly a label but rather the original unaugmented text to be passed into the model to get a standard prediction to compare against). The way I got around this was to use a custom batch sampler which produces indices of data that will be in a batch, containing 2 batch samplers, one for unlabeled augmented data and labeled data, and then zipping the two batch samplers to concatenate them. The data would be arranged such that the first range of indices would be occupied by labeled data, and the next rangeunlabeled augmented, and the last range the original unlabeled, so each batch sample would sample, then the custom sampler will append and fix the offsets so that the indices are correct for the whole dataset. There is also another set of indices appended after the labeled+unlabeledaug belonging to the original unlabeled data, offset approprately. Note that this will make it such that the indices come at a seemingly larger batch size.\n","\n","\n","Then the collate function will pad and collate the batches of text, then split the batch into the labeled+unlabeledaug and a unlabeledori batch as what the model needs to backpropagate from are the labeled data and the unlabeled augmented set. The unlabeled original batch will be returned as the target instead (in a tuple with the labeled target), to be further processed by the callback that governs the entire implementation.\n","\n","The loss involves two components, the supervised loss from the labeled data, and the unsupervised loss from the unlabeled data. This had to be implemented by using a custom loss to convert the logits predicted by the model into the log probability that the loss took as input.\n","\n","The callback handled the replacement of the various parts of the pipeline, as well as shaping the data appropriately to be passed into the next part. The calculation of the original unlabeled probabilities as well as the TSA threshold mask takes place here.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LHMTN6Mi8oiC","colab_type":"text"},"source":["##Quirks of Colab\n","\n","The more you use their GPU the more likely for them to time you out/ give you worse GPUs\n","Workaround: Make more google accounts \n","\n","Free google drive storage may not be able to save model checkpoints from every epoch  \n","Workaround: Download to hard drive first\n","\n","Random disconnects when tabbing out\n","Workaround: \n","```\n","function ConnectButton(){\n","    console.log(\"Connect pushed\"); \n","    document.querySelector(\"#connect\").click() \n","}\n","setInterval(ConnectButton,120000);\n","```\n","Run this javascript code to auto click the connect button. Seems to work most of the time.\n","\n","Somehow I feel that when running two runtimes on different accounts for the same file will interfere with each other."]},{"cell_type":"markdown","metadata":{"id":"Vc_y4Po7bFKf","colab_type":"text"},"source":["#Future work\n","\n","Try to give stuff other than the classic \"more data/compute\"\n","But still more compute - 1 GPU with 15 Gb of useable ram not cutting it, especially when the model itself is 1 Gb. Have to use smaller batchsizes, which will affect training time, and may even affect the accuracy - especially in the case of UDA, where the ratio of unlabeled to labeled data per batch is important as that determines whether the model is able to look at every datapoint we have.\n","\n","Cleaner data - Instead of splitting into sentences before the cleaning, should clean first to avoid markdown being screwed up  \n","\n","MixMatch - another semi-supervised learning algorithm. Use MixUp on the numericalised text or embeddings maybe\n","\n","Many hyperparameters to tune: Didnt really vary much on the weight decay; many other combinations of parameters to tune  \n","e.g. UDA schedule vs weight decay vs learning rate or looking at which language model would give the best result or testing UDA without language model pretraining\n","\n","Combined model using both text and features - May give better results. Can implement in fast.ai using link in notebook, but need to mess around with custom everything for it to work. \n","\n","Could have predicted each label individually -  Supportiveness and Disclosure not exactly linked \n","\n","Use full roberta model - had to use a distilled version as it had much lower memory requirements (in exchange for losing a bit of accuracy)\n","\n","More epochs for certain models - Some models may be underfitting since the validation loss was still decreasing."]}]}